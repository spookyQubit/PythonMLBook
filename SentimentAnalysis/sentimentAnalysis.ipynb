{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis using IMDB data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the IMDB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0%                          100%\n",
      "[##############################] | ETA[sec]: 0.000 \n",
      "Total time elapsed: 458.021 sec\n"
     ]
    }
   ],
   "source": [
    "# Data from Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. (2011). \n",
    "# Learning Word Vectors for Sentiment Analysis. \n",
    "# The 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011).\n",
    "\n",
    "import pyprind \n",
    "import pandas as pd\n",
    "import os \n",
    "pybar = pyprind.ProgBar(50000) # 50000 is the number fo files.\n",
    "\n",
    "label = {'pos':1, 'neg':0}\n",
    "df_train = pd.DataFrame()\n",
    "df_test  = pd.DataFrame()\n",
    "for testTrainFolders in ['test', 'train']: # Load only the test data\n",
    "    for classesFolder in ['pos', 'neg']:\n",
    "        path = './aclImdb/%s/%s' %(testTrainFolders, classesFolder)\n",
    "        for f in os.listdir(path):\n",
    "            with open(os.path.join(path, f), 'r') as infile:\n",
    "                text = infile.read()\n",
    "                if testTrainFolders == \"test\":\n",
    "                    df_test = df_test.append([[text, label[classesFolder]]], ignore_index = True)\n",
    "                    pybar.update()\n",
    "                elif testTrainFolders == \"train\":\n",
    "                    df_train = df_train.append([[text, label[classesFolder]]], ignore_index = True)\n",
    "                    pybar.update()\n",
    "                \n",
    "df_test.columns = ['review', 'sentiment']\n",
    "df_train.columns = ['review', 'sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomize the test data and store it in csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review  sentiment\n",
      "0  This is a generally nice film, with good story...          1\n",
      "1  I just accidentally stumbled over this film on...          1\n",
      "2  This movie is a good example of the extreme la...          0\n",
      "                                              review  sentiment\n",
      "0  hello all Denver fans!<br /><br />i couldn't a...          1\n",
      "1  If you watched this film for the nudity (as I ...          1\n",
      "2  The opening scene keeps me from rating at abso...          0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "df_train = df_train.reindex(np.random.permutation(df_train.index))\n",
    "df_train.to_csv('./movie_train_data.csv', index=False)\n",
    "df_train = pd.read_csv('./movie_train_data.csv')\n",
    "\n",
    "df_test  = df_test.reindex(np.random.permutation(df_test.index))\n",
    "df_test.to_csv('./movie_test_data.csv', index=False)\n",
    "df_test = pd.read_csv('./movie_test_data.csv')\n",
    "\n",
    "print df_train.head(3)\n",
    "print df_test.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer for getting the text as feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 0 0 0 0 1 0 0 1 1 0]\n",
      " [0 0 1 0 0 0 0 0 0 1 1 1 1]\n",
      " [1 0 0 1 1 1 1 0 1 0 1 2 0]]\n",
      "{u'land': 5, u'rises': 7, u'sun': 10, u'is': 3, u'sets': 9, u'of': 6, u'rising': 8, u'west': 12, u'in': 2, u'japan': 4, u'the': 11, u'east': 1, u'called': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "ctVectorizer = CountVectorizer()\n",
    "allData = ['Sun rises in the east',\n",
    "          'Sun sets in the west',\n",
    "          'Japan is called the land of the rising sun.']\n",
    "featureVectors = ctVectorizer.fit_transform(allData)\n",
    "print featureVectors.toarray() \n",
    "print ctVectorizer.vocabulary_  # Note that CountVectorizer is case insensitive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term frequency and Inverse-term frequency: TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.55249005  0.42018292  0.          0.          0.          0.\n",
      "   0.55249005  0.          0.          0.32630952  0.32630952  0.        ]\n",
      " [ 0.          0.          0.42018292  0.          0.          0.          0.\n",
      "   0.          0.          0.55249005  0.32630952  0.32630952  0.55249005]\n",
      " [ 0.35934656  0.          0.          0.35934656  0.35934656  0.35934656\n",
      "   0.35934656  0.          0.35934656  0.          0.21223587  0.42447173\n",
      "   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer()\n",
    "norm_featureVectors = transformer.fit_transform(featureVectors.toarray())\n",
    "print norm_featureVectors.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term frequency and Inverse-term frequency: TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.55249005  0.42018292  0.          0.          0.          0.\n",
      "   0.55249005  0.          0.          0.32630952  0.32630952  0.        ]\n",
      " [ 0.          0.          0.42018292  0.          0.          0.          0.\n",
      "   0.          0.          0.55249005  0.32630952  0.32630952  0.55249005]\n",
      " [ 0.35934656  0.          0.          0.35934656  0.35934656  0.35934656\n",
      "   0.35934656  0.          0.35934656  0.          0.21223587  0.42447173\n",
      "   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfVectorizer = TfidfVectorizer()\n",
    "print tfVectorizer.fit_transform(allData).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning data: Remove http tags and any non word characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "def preprocessor(text):\n",
    "    text = re.sub(r'<[^>]*>', '', text) # Removing the http tags\n",
    "    text = re.sub(r'\\W+', ' ', text) # Removing all non words (even emoticons)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Porter tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'fast', u'fast', u'fast']\n",
      "[[ 0.  1.]\n",
      " [ 1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]\n",
    "\n",
    "print tokenizer_porter(\"fast fasts fasting\")\n",
    "\n",
    "tfVectorizer = TfidfVectorizer(tokenizer=tokenizer_porter)\n",
    "print tfVectorizer.fit_transform(['run runs running', 'fast fasts fasting']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to fit\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Done fitting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:  4.6min finished\n"
     ]
    }
   ],
   "source": [
    "# Recap\n",
    "# Data is in df_test. This was read from the files and then shuffled (shuffling was not needed because we are not mixing test and training data.)\n",
    "# Use TfidfVectorizer to get the feature vectors ==> Use porterStemmer as tokenizer.\n",
    "# Use logistic regression for fitting. \n",
    "# Use the learned logistic regression to predict.\n",
    "\n",
    "# Before we create the pipeline, we need to get the data in numpy array from df_test.\n",
    "# X_test should be a list of strings. This will get converted to n_sample X n_features numpy_array by TfidfVectorizer.\n",
    "# y_test sould be n_sample numpy array.\n",
    "\n",
    "X_train = df_train.loc[:10000, \"review\"].values     ## 25000 data points taking too much time. \n",
    "y_train = df_train.loc[:10000, \"sentiment\"].values\n",
    "\n",
    "X_test = df_test.loc[:10000, \"review\"].values\n",
    "y_test = df_test.loc[:10000, \"sentiment\"].values\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "\n",
    "#param_grid = [{'vect__ngram_range'  : [(1,1)], \n",
    "#               'vect__tokenizer'    : [tokenizer_porter, tokenizer],\n",
    "#               'vect__preprocessor' : [None, preprocessor],\n",
    "#               'lr__penalty'        : ['l2'],\n",
    "#               'lr__C'              : [1.0, 10]}]\n",
    "\n",
    "param_grid = [{'vect__tokenizer'    : [tokenizer_porter, tokenizer],\n",
    "               'lr__C'              : [1.0, 10]}]\n",
    "\n",
    "tfidf_lr_pipeline = Pipeline([('vect', TfidfVectorizer(preprocessor=preprocessor)), \n",
    "                              ('lr', LogisticRegression(random_state=0))])\n",
    "gs_tfidf_lr = GridSearchCV(tfidf_lr_pipeline, \n",
    "                           param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           cv=5, \n",
    "                           verbose=1)\n",
    "\n",
    "print \"Starting to fit\"\n",
    "gs = gs_tfidf_lr.fit(X_train, y_train )\n",
    "print \"Done fitting\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score 0.879\n",
      "Best parameters {'vect__tokenizer': <function tokenizer at 0x8f30758>, 'lr__C': 10}\n",
      "Test accuracy 0.877\n"
     ]
    }
   ],
   "source": [
    "print (\"Best score %.3f\" % gs.best_score_)\n",
    "print (\"Best parameters %s\" % gs.best_params_)\n",
    "\n",
    "clf = gs.best_estimator_\n",
    "print (\"Test accuracy %.3f\" % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
